{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a LIFO storage system that will push out the oldest entries if it exceeds the capacity\n",
    "class ReplayMemory(deque):\n",
    "    def __init__(self, capacity):\n",
    "        super().__init__(maxlen=capacity)\n",
    "            \n",
    "    def sample(self, amt_sample):\n",
    "        return random.sample(self, amt_sample)\n",
    "    \n",
    "    def store_transition(self, trans):\n",
    "        self.append(trans)\n",
    "    \n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5) # 3 in channels \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16*13*13 , 32)  \n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*13*13)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, obs_space, action_space, config):\n",
    "        self.config = config\n",
    "        \n",
    "        self.observation_space = obs_space\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.epsilon = self.config[\"EPS\"]\n",
    "        self.epsilon_decay = self.config[\"EPS_DECAY\"]\n",
    "        self.gamma = self.config[\"GAMMA\"]\n",
    "        \n",
    "        self.policy_network = CNN().to(device)\n",
    "        self.target_network = CNN().to(device)\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "        \n",
    "        self.optimizer = optim.SGD(self.policy_network.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "        self.memory = ReplayMemory(self.config[\"MEMORY_CAPACITY\"])\n",
    "        self.batch_size = self.config[\"BATCH_SIZE\"]\n",
    "        \n",
    "    def step(self, obs):\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action_tensor = self.policy_network(obs).clone()\n",
    "                action_tensor[action_tensor > 0] = 1\n",
    "                action_tensor[action_tensor <= 0] = 0\n",
    "                action = {\n",
    "                    'attack': action_tensor[0][0].item(),\n",
    "                    'back': action_tensor[0][1].item(),\n",
    "                    'camera': {\n",
    "                        'look_up': action_tensor[0][2].item(),\n",
    "                        'look_down': action_tensor[0][3].item(),\n",
    "                        'look_right': action_tensor[0][4].item(),\n",
    "                        'look_left': action_tensor[0][5].item()\n",
    "                    },\n",
    "                    'forward': action_tensor[0][6].item(),\n",
    "                    'jump': action_tensor[0][7].item(),\n",
    "                    'left': action_tensor[0][8].item(),\n",
    "                    'right': action_tensor[0][9].item(),\n",
    "                    'sneak': action_tensor[0][10].item(),\n",
    "                    'sprint': action_tensor[0][11].item()\n",
    "                }\n",
    "        return action\n",
    "  \n",
    "    def evaluate(self, old_state, action_taken, new_state, reward):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        else:\n",
    "            transitions = self.memory.sample(self.batch_size)\n",
    "\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            all_states = torch.cat(batch.state)\n",
    "            all_actions = torch.cat(batch.action)\n",
    "            all_rewards = torch.cat(batch.reward)\n",
    "            all_next_states = torch.cat([s for s in batch.next_state if s is not None])      \n",
    "\n",
    "            #This will help us prevent the calculation for y when the final state is none... not working atm\n",
    "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "            policy_q_table = self.policy_network(all_states)\n",
    "            actual = torch.einsum('ij, ij->i', policy_q_table, all_actions)\n",
    "\n",
    "            next_state_max_q = torch.zeros(self.batch_size, device=device)\n",
    "            #will be zero on final states\n",
    "            target_q_table = self.target_network(all_next_states)\n",
    "\n",
    "            next_state_max_q[non_final_mask] = torch.sum(target_q_table[target_q_table > 0])\n",
    "\n",
    "            y = (all_rewards + self.gamma *next_state_max_q)\n",
    "\n",
    "            loss = F.l1_loss(actual, y)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for param in self.policy_network.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def agent_sync_networks(self):\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "            \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = np.clip(self.epsilon * self.epsilon_decay, .01, .99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
