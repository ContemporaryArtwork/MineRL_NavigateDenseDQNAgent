{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple for label information\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "# The following is a FIFO storage system that will push out the oldest entries if it exceeds the capacity\n",
    "class ReplayMemory(deque):\n",
    "    def __init__(self, capacity):\n",
    "        super().__init__(maxlen=capacity)\n",
    "            \n",
    "    def sample(self, amt_sample):\n",
    "        return random.sample(self, amt_sample)\n",
    "    \n",
    "    #expects to take in numpy or numeric values\n",
    "    def store_transition(self, state, action, next_state, reward, done):\n",
    "        trans = Transition(\n",
    "            state,\n",
    "            action,\n",
    "            next_state,\n",
    "            reward,\n",
    "            done\n",
    "        )\n",
    "        self.append(trans)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d583208c2e00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mCNNNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCNNNetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 3 in channels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class CNNNetwork(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(CNNNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5) # 3 in channels \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16*13*13, 32)  # 4*4 from grid world size\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, output_dim)\n",
    "\n",
    "    def forward(self, pov):\n",
    "        x = self.pool(F.relu(self.conv1(pov)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*13*13)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        #used to process the POV\n",
    "        self.cnn = CNNNetwork(1)\n",
    "        \n",
    "        #output of CNN + compass + actions\n",
    "        self.action_dim = action_dim\n",
    "        self.fc1 = nn.Linear(1 + 4 + action_dim, 1)\n",
    "\n",
    "    def forward(self, pov, compass, action):\n",
    "        x = self.cnn.forward(pov)\n",
    "        compass = compass.view(-1, 1)\n",
    "        action = action.view(-1, self.action_dim)\n",
    "        x = self.fc1(torch.cat((x, compass, compass, compass, compass, action),dim=1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        #used to process the POV\n",
    "        self.cnn = CNNNetwork(1)\n",
    "        #output of CNN + compass \n",
    "        self.fc1 = nn.Linear(1 + 4, action_dim)\n",
    "        \n",
    "        #for calculation of a normal distribution\n",
    "        self.avg_out = nn.Linear(action_dim, action_dim)\n",
    "        self.dev_out = nn.Linear(action_dim, action_dim)\n",
    "\n",
    "    def forward(self, pov, compass):\n",
    "        x = self.cnn.forward(pov)\n",
    "        compass = compass.view(-1, 1)\n",
    "        x = self.fc1(torch.cat((x, compass, compass, compass, compass),dim=1))\n",
    "        avg = self.avg_out(x)\n",
    "        log_dev = self.dev_out(x)\n",
    "        #clamp the variance so that it is assured between ~ 2e-9 and 7.4\n",
    "        log_dev = torch.clamp(log_dev, -20, 2)\n",
    "        dev = torch.exp(log_dev)\n",
    "\n",
    "        # Pre-squash distribution and sample\n",
    "        distribution = Normal(avg, dev)\n",
    "        action = distribution.rsample()\n",
    "        \n",
    "        logp = self.calc_logp(action, distribution)\n",
    "\n",
    "        #squash the action so that it can only be (-180, 180)\n",
    "        action = self.squash(action, 30)\n",
    "        \n",
    "        return action, logp\n",
    "    \n",
    "    def squash(self, action, range):\n",
    "        return torch.tanh(action) * range\n",
    "    \n",
    "    # This is equation 21 from the original SAC paper in Appendix C\n",
    "    def calc_logp(self, action, distribution):\n",
    "        #this prevents the equation from approaching -inf\n",
    "        epsilon = .0001\n",
    "        return (distribution.log_prob(action) - torch.log(1 - torch.square(torch.tanh(action)) + epsilon)).sum(axis=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftACAgent():\n",
    "    def __init__(self, env, config):\n",
    "        self.config = config\n",
    "        \n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "        \n",
    "        self.alpha = self.config[\"ALPHA\"]\n",
    "        self.gamma = self.config[\"GAMMA\"]\n",
    "        self.polyak = self.config[\"POLYAK\"]\n",
    "        \n",
    "        self.memory = ReplayMemory(self.config[\"MAX_MEMORY\"])\n",
    "        self.batch_size = self.config[\"BATCH_SIZE\"]\n",
    "        \n",
    "        flat_in = gym.spaces.flatten_space(self.observation_space).shape[0]\n",
    "        flat_out = gym.spaces.flatten_space(self.action_space).shape[0]\n",
    "        \n",
    "        #responsible for estimating our policy\n",
    "        self.actor = ActorNetwork(1).to(device)\n",
    "        \n",
    "        #our critic is estimating the action value\n",
    "        self.critic1 = CriticNetwork(1).to(device)\n",
    "        self.critic2 = deepcopy(self.critic1)\n",
    "        self.critic1_target = deepcopy(self.critic1)\n",
    "        self.critic2_target = deepcopy(self.critic2)\n",
    "        \n",
    "        for p in self.critic1_target.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.critic2_target.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        self.optimizer_actor = optim.SGD(self.actor.parameters(), lr=self.config[\"LEARNING_RATE\"], momentum=0.9)\n",
    "        self.optimizer_critic = optim.SGD(self.critic1.parameters(), lr=self.config[\"LEARNING_RATE\"], momentum=0.9)\n",
    "\n",
    "        \n",
    "    def step(self, obs):\n",
    "        with torch.no_grad():\n",
    "            action, _ = self.actor(obs[\"pov\"], obs[\"compassAngle\"])\n",
    "        return action\n",
    "  \n",
    "    def evaluate(self):\n",
    "        try:\n",
    "            transitions = self.memory.sample(self.batch_size)\n",
    "        except ValueError:\n",
    "            return False\n",
    "        \n",
    "        # manipulate transitions\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "\n",
    "        pov = torch.cat([state[\"pov\"] for state in batch.state])\n",
    "        compass = torch.cat([state[\"compassAngle\"] for state in batch.state])\n",
    "        actions = torch.cat(batch.action)\n",
    "        next_pov = torch.cat([state[\"pov\"] for state in batch.next_state])\n",
    "        next_compass = torch.cat([state[\"compassAngle\"] for state in batch.next_state])\n",
    "        rewards = torch.cat(batch.reward)\n",
    "        done_signals = torch.cat(batch.done)\n",
    "        \n",
    "        \n",
    "        # optimize the critic\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        critic_loss = self.compute_loss_critic(pov, compass, actions, next_pov, next_compass, rewards, done_signals)\n",
    "        critic_loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "        \n",
    "        # Freeze Q-networks so you don't waste computational effort \n",
    "        # computing gradients for them during the policy learning step.\n",
    "        for p in self.critic1.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.critic2.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        # optimize the critic\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        loss_actor = self.compute_loss_actor(pov, compass)\n",
    "        loss_actor.backward()\n",
    "        self.optimizer_actor.step()\n",
    "        \n",
    "        for p in self.critic1.parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in self.critic2.parameters():\n",
    "            p.requires_grad = True\n",
    "            \n",
    "        # Finally, update target networks by polyak averaging.\n",
    "        with torch.no_grad():\n",
    "            for c1,c1t,c2,c2t in zip(\n",
    "                    self.critic1.parameters(),\n",
    "                    self.critic1_target.parameters(),\n",
    "                    self.critic2.parameters(),\n",
    "                    self.critic2_target.parameters()\n",
    "            ):\n",
    "                c1t.data.mul_(self.polyak)\n",
    "                c1t.data.add_((1 - self.polyak) * c1.data)\n",
    "                c2t.data.mul_(self.polyak)\n",
    "                c2t.data.add_((1 - self.polyak) * c2.data)\n",
    "        \n",
    "\n",
    "    # This function covers line 12 from the Spinning Up Article psedeocode\n",
    "    def compute_loss_critic(self, pov, compass, actions, next_pov, next_compass, rewards, done_signals):\n",
    "        q1 = self.critic1(pov, compass, actions)\n",
    "        q2 = self.critic2(pov, compass, actions)\n",
    "        \n",
    "        # Bellman backup for Q functions\n",
    "        with torch.no_grad():\n",
    "            # Target actions come from *current* policy\n",
    "            target_actions, log_probs = self.actor(next_pov, next_compass)\n",
    "                                        \n",
    "            # Target Q-values\n",
    "            q1_targ = self.critic1_target(next_pov, next_compass, target_actions)\n",
    "            q2_targ = self.critic2_target(next_pov, next_compass, target_actions)\n",
    "            q_targ = torch.min(q1_targ, q2_targ)\n",
    "            \n",
    "            y = rewards + self.gamma * (1 - done_signals) * (q_targ - self.alpha * log_probs)\n",
    "        \n",
    "        loss_critic1 = ((q1-y)**2).mean()\n",
    "        loss_critic2 = ((q2-y)**2).mean()\n",
    "        loss_critic = loss_critic1 + loss_critic2\n",
    "        \n",
    "        return loss_critic\n",
    "    \n",
    "    # This function covers line 14 from the Spinning Up Article psedeocode\n",
    "    def compute_loss_actor(self, pov, compass):\n",
    "        actions, log_probs = self.actor(pov, compass)\n",
    "        \n",
    "        q1_targ = self.critic1_target(pov, compass, actions)\n",
    "        q2_targ = self.critic2_target(pov, compass, actions)\n",
    "        q_targ = torch.min(q1_targ, q2_targ)\n",
    "        \n",
    "        # Entropy-regularized policy loss\n",
    "        loss_actor = (self.alpha * log_probs - q_targ).mean()\n",
    "\n",
    "        return loss_actor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
