{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple for label information\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "# The following is a FIFO storage system that will push out the oldest entries if it exceeds the capacity\n",
    "class ReplayMemory(deque):\n",
    "    def __init__(self, capacity):\n",
    "        super().__init__(maxlen=capacity)\n",
    "            \n",
    "    def sample(self, amt_sample):\n",
    "        return random.sample(self, amt_sample)\n",
    "    \n",
    "    #expects to take in numpy or numeric values\n",
    "    def store_transition(self, state, action, next_state, reward, done):\n",
    "        trans = Transition(\n",
    "            torch.tensor(state, device=device).float().unsqueeze(0), \n",
    "            torch.tensor([action], device=device).float().unsqueeze(0),\n",
    "            torch.tensor(next_state, device=device).float().unsqueeze(0) if next_state is not None else None, \n",
    "            torch.tensor([reward], device=device).float().unsqueeze(0),\n",
    "            torch.tensor([done], device=device).int().unsqueeze(0)\n",
    "        )\n",
    "        self.append(trans)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(CNNNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5) # 3 in channels \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16*13*13, 32)  # 4*4 from grid world size\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16 + 1, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #insert compass at end of the linear layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNetwork(CNNNetwork):\n",
    "    def forward(self, x):\n",
    "        x = Linear.forward(self, x)\n",
    "        #We need to change the distribution and make a parameterization trick\n",
    "        return Categorical(F.softmax(x, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftACAgent():\n",
    "    def __init__(self, env, config):\n",
    "        self.config = config\n",
    "        \n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "        \n",
    "        self.alpha = self.config[\"ALPHA\"]\n",
    "        self.gamma = self.config[\"GAMMA\"]\n",
    "        self.polyak = self.config[\"POLYAK\"]\n",
    "        \n",
    "        self.memory = ReplayMemory(self.config[\"MAX_MEMORY\"])\n",
    "        self.batch_size = self.config[\"BATCH_SIZE\"]\n",
    "        \n",
    "        flat_in = gym.spaces.flatten_space(self.observation_space).shape[0]\n",
    "        flat_out = gym.spaces.flatten_space(self.action_space).shape[0]\n",
    "        \n",
    "        print(gym.spaces.flatten_space(self.observation_space))\n",
    "        \n",
    "        #responsible for estimating our policy\n",
    "        self.actor = LinearDistribution(flat_in, flat_out).to(device)\n",
    "        \n",
    "        #our critic is estimating the action value\n",
    "        self.critic1 = Linear(flat_in, flat_out).to(device)\n",
    "        self.critic2 = deepcopy(self.critic1)\n",
    "        self.critic1_target = deepcopy(self.critic1)\n",
    "        self.critic2_target = deepcopy(self.critic2)\n",
    "        \n",
    "        for p in self.critic1_target.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.critic2_target.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        self.optimizer_actor = optim.SGD(self.actor.parameters(), lr=self.config[\"LEARNING_RATE\"], momentum=0.9)\n",
    "        self.optimizer_critic = optim.SGD(self.critic1.parameters(), lr=self.config[\"LEARNING_RATE\"], momentum=0.9)\n",
    "\n",
    "        \n",
    "    def step(self, obs):\n",
    "        with torch.no_grad():\n",
    "            obs = torch.tensor(obs, device=device).float()\n",
    "            policy_distribution = self.actor(obs)\n",
    "            action = policy_distribution.sample().item()\n",
    "        return action\n",
    "  \n",
    "    def evaluate(self):\n",
    "        try:\n",
    "            transitions = self.memory.sample(self.batch_size)\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "        # manipulate transitions\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        states = torch.cat(batch.state)\n",
    "        actions = torch.cat(batch.action)\n",
    "        next_states = torch.cat(batch.next_state)\n",
    "        rewards = torch.cat(batch.reward)\n",
    "        done_signals = torch.cat(batch.done)\n",
    "        \n",
    "        \n",
    "        # optimize the critic\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        critic_loss = self.compute_loss_critic(states, actions, next_states, rewards, done_signals)\n",
    "        critic_loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "        \n",
    "        # Freeze Q-networks so you don't waste computational effort \n",
    "        # computing gradients for them during the policy learning step.\n",
    "        for p in self.critic1.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.critic2.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        # optimize the critic\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        loss_actor = self.compute_loss_actor(states)\n",
    "        loss_actor.backward()\n",
    "        self.optimizer_actor.step()\n",
    "        \n",
    "        for p in self.critic1.parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in self.critic2.parameters():\n",
    "            p.requires_grad = True\n",
    "            \n",
    "        # Finally, update target networks by polyak averaging.\n",
    "        with torch.no_grad():\n",
    "            for c1,c1t,c2,c2t in zip(\n",
    "                    self.critic1.parameters(),\n",
    "                    self.critic1_target.parameters(),\n",
    "                    self.critic2.parameters(),\n",
    "                    self.critic2_target.parameters()\n",
    "            ):\n",
    "                c1t.data.mul_(self.polyak)\n",
    "                c1t.data.add_((1 - self.polyak) * c1.data)\n",
    "                c2t.data.mul_(self.polyak)\n",
    "                c2t.data.add_((1 - self.polyak) * c2.data)\n",
    "        \n",
    "\n",
    "    # This function covers line 12 from the Spinning Up Article psedeocode\n",
    "    def compute_loss_critic(self, states, actions, next_states, rewards, done_signals):\n",
    "        ind_actions = actions.long()\n",
    "        q1 = self.critic1(states).gather(1, ind_actions)\n",
    "        q2 = self.critic2(states).gather(1, ind_actions)\n",
    "        \n",
    "        \n",
    "        # Bellman backup for Q functions\n",
    "        with torch.no_grad():\n",
    "            # Target actions come from *current* policy\n",
    "            target_distribution = self.actor(next_states)\n",
    "            target_actions = target_distribution.sample()\n",
    "            log_probs = target_distribution.log_prob(target_actions).unsqueeze(1)\n",
    "            ind_target_actions = target_actions.long().unsqueeze(1)\n",
    "                                        \n",
    "            # Target Q-values\n",
    "            q1_targ = self.critic1_target(next_states).gather(1, ind_target_actions)\n",
    "            q2_targ = self.critic2_target(next_states).gather(1, ind_target_actions)\n",
    "            q_targ = torch.min(q1_targ, q2_targ)\n",
    "            \n",
    "            y = rewards + self.gamma * (1 - done_signals) * (q_targ - self.alpha * log_probs)\n",
    "        \n",
    "        loss_critic1 = ((q1-y)**2).mean()\n",
    "        loss_critic2 = ((q2-y)**2).mean()\n",
    "        loss_critic = loss_critic1 + loss_critic2\n",
    "        \n",
    "        return loss_critic\n",
    "    \n",
    "    # This function covers line 14 from the Spinning Up Article psedeocode\n",
    "    def compute_loss_actor(self, states):\n",
    "        current_distribution = self.actor(states)\n",
    "        actions = current_distribution.sample()\n",
    "        log_probs = current_distribution.log_prob(actions).unsqueeze(1)\n",
    "        ind_actions = actions.long().unsqueeze(1)\n",
    "        \n",
    "        q1_targ = self.critic1_target(states).gather(1, ind_actions)\n",
    "        q2_targ = self.critic2_target(states).gather(1, ind_actions)\n",
    "        q_targ = torch.min(q1_targ, q2_targ)\n",
    "        \n",
    "        # Entropy-regularized policy loss\n",
    "        loss_actor = (self.alpha * log_probs - q_targ).mean()\n",
    "\n",
    "        return loss_actor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
